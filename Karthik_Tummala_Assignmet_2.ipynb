{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarthikTummala18/INFO-5731-Computational-Methods-for-Information-Systems-/blob/main/Karthik_Tummala_Assignmet_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C7LaxsRR6aiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "main_url = \"https://ddr.densho.org/narrators/?page={}\"\n",
        "\n",
        "\n",
        "narrators = []\n",
        "\n",
        "\n",
        "for page_num in range(1, 41):\n",
        "    link1 = Request(main_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    url1 = urlopen(link1)\n",
        "    data1 = url1.read()\n",
        "    data1_soup = BeautifulSoup(data1, 'html.parser')\n",
        "\n",
        "\n",
        "    narrators_list = data1_soup.find_all('h4', class_='media-heading')\n",
        "\n",
        "    for narrator in narrators_list:\n",
        "\n",
        "        narrator_link = narrator.find('a')['href']\n",
        "        narrator_url = narrator_link\n",
        "\n",
        "\n",
        "        link2 = Request(narrator_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        url2 = urlopen(link2)\n",
        "        data2 = url2.read()\n",
        "        data2_soup = BeautifulSoup(data2, 'html.parser')\n",
        "        narrator_name = data2_soup.find('h1').get_text(strip=True)\n",
        "        bio = data2_soup.find('p').get_text(strip=True)\n",
        "        interview_divs = data2_soup.find_all('div', class_='media-body')\n",
        "\n",
        "        for div in interview_divs:\n",
        "            interview_title = div.find('b', class_='media-heading').get_text(strip=True)\n",
        "            interview_details = div.find('div', class_='source muted').get_text(strip=True)\n",
        "            interview_link = div.find('div', class_='url').get_text(strip=True)\n",
        "            narrators.append({\n",
        "                'name': narrator_name,\n",
        "                'bio': bio,\n",
        "                'interview_title': interview_title,\n",
        "                'interview_details': interview_details,\n",
        "                'interview_link': interview_link,\n",
        "                'narrator_url': narrator_url\n",
        "            })\n",
        "\n",
        "\n",
        "csv_filename = 'narrators_data.csv'\n",
        "\n",
        "\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['name', 'bio', 'interview_title', 'interview_details', 'interview_link', 'narrator_url']\n",
        "\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for narrator in narrators:\n",
        "        writer.writerow({\n",
        "            'name': narrator['name'],\n",
        "            'bio': narrator['bio'],\n",
        "            'interview_title': narrator['interview_title'],\n",
        "            'interview_details': narrator['interview_details'],\n",
        "            'interview_link': narrator['interview_link'],\n",
        "            'narrator_url': narrator['narrator_url']\n",
        "        })\n",
        "\n",
        "\n",
        "print(f\"Total narrators collected: {len(narrators)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ptRluMuRftv",
        "outputId": "e51ba817-3adf-4204-b1c2-e817374d91b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9Q_wHDakWFW"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('narrators_data.csv')\n",
        "\n",
        "df['bio'] = df['bio'].fillna('')\n",
        "df['interview_details'] = df['interview_details'].fillna('')\n",
        "\n",
        "def remove_noise(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def remove_numbers(text):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    text = \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    return text\n",
        "\n",
        "def to_lowercase(text):\n",
        "    if isinstance(text, str):\n",
        "        return text.lower()\n",
        "    return text\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "def stemming(text):\n",
        "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['cleaned_name'] = df['name'].apply(to_lowercase)\n",
        "df['cleaned_bio'] = df['bio'].apply(remove_noise)\n",
        "df['cleaned_bio'] = df['cleaned_bio'].apply(remove_numbers)\n",
        "df['cleaned_bio'] = df['cleaned_bio'].apply(remove_stopwords)\n",
        "df['cleaned_bio'] = df['cleaned_bio'].apply(to_lowercase)\n",
        "df['cleaned_bio'] = df['cleaned_bio'].apply(stemming)\n",
        "df['cleaned_bio'] = df['cleaned_bio'].apply(lemmatization)\n",
        "\n",
        "df['cleaned_interview_details'] = df['interview_details'].apply(remove_noise)\n",
        "df['cleaned_interview_details'] = df['cleaned_interview_details'].apply(remove_numbers)\n",
        "df['cleaned_interview_details'] = df['cleaned_interview_details'].apply(remove_stopwords)\n",
        "df['cleaned_interview_details'] = df['cleaned_interview_details'].apply(to_lowercase)\n",
        "df['cleaned_interview_details'] = df['cleaned_interview_details'].apply(stemming)\n",
        "df['cleaned_interview_details'] = df['cleaned_interview_details'].apply(lemmatization)\n",
        "\n",
        "df.to_csv('narrators_data_cleaned.csv', index=False)\n",
        "\n",
        "print(df[['name', 'cleaned_name','cleaned_bio', 'cleaned_interview_details']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts7yBN1qUJRP",
        "outputId": "45b5ec58-27f8-43cb-9b72-505b355955ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    name           cleaned_name  \\\n",
            "0           Kay Aiko Abe           kay aiko abe   \n",
            "1                Art Abe                art abe   \n",
            "2  Sharon Tanagi Aburano  sharon tanagi aburano   \n",
            "3  Sharon Tanagi Aburano  sharon tanagi aburano   \n",
            "4        Toshiko Aiboshi        toshiko aiboshi   \n",
            "\n",
            "                                         cleaned_bio  \\\n",
            "0  nisei femal born may selleck washington spent ...   \n",
            "1  nisei male born june seattl washington grew ar...   \n",
            "2  nisei femal born octob seattl washington famil...   \n",
            "3  nisei femal born octob seattl washington famil...   \n",
            "4  nisei femal born juli boyl height california e...   \n",
            "\n",
            "                cleaned_interview_details  \n",
            "0        decemb seattl washington segment  \n",
            "1       januari seattl washington segment  \n",
            "2         april seattl washington segment  \n",
            "3         march seattl washington segment  \n",
            "4  januari culver citi california segment  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/narrators_data_cleaned.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYafJb1dQdsh",
        "outputId": "b60a3ac6-ff60-45eb-fc05-0cebc0b7e27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    name                                                bio  \\\n",
            "0           Kay Aiko Abe  Nisei female. Born May 9, 1927, in Selleck, Wa...   \n",
            "1                Art Abe  Nisei male. Born June 12, 1921, in Seattle, Wa...   \n",
            "2  Sharon Tanagi Aburano  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "3  Sharon Tanagi Aburano  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "4        Toshiko Aiboshi  Nisei female. Born July 8, 1928, in Boyle Heig...   \n",
            "\n",
            "                                     interview_title  \\\n",
            "0       Kay Aiko Abe Interview — ddr-densho-1000-232   \n",
            "1            Art Abe Interview — ddr-densho-1000-206   \n",
            "2  Sharon Tanagi Aburano Interview II — ddr-densh...   \n",
            "3  Sharon Tanagi Aburano Interview I — ddr-densho...   \n",
            "4         Toshiko Aiboshi Interview — ddr-manz-1-112   \n",
            "\n",
            "                                   interview_details  \\\n",
            "0  December 2, 2008.\\n      Seattle, Washington.0...   \n",
            "1  January 24, 2008.\\n      Seattle, Washington.0...   \n",
            "2  April 3, 2008.\\n      Seattle, Washington.02:2...   \n",
            "3  March 25, 2008.\\n      Seattle, Washington.02:...   \n",
            "4  January 20, 2011.\\n      Culver City, Californ...   \n",
            "\n",
            "                                interview_link  \\\n",
            "0  https://ddr.densho.org/ddr-densho-1000-232/   \n",
            "1  https://ddr.densho.org/ddr-densho-1000-206/   \n",
            "2  https://ddr.densho.org/ddr-densho-1000-209/   \n",
            "3  https://ddr.densho.org/ddr-densho-1000-208/   \n",
            "4       https://ddr.densho.org/ddr-manz-1-112/   \n",
            "\n",
            "                            narrator_url           cleaned_name  \\\n",
            "0  https://ddr.densho.org/narrators/361/           kay aiko abe   \n",
            "1  https://ddr.densho.org/narrators/291/                art abe   \n",
            "2  https://ddr.densho.org/narrators/293/  sharon tanagi aburano   \n",
            "3  https://ddr.densho.org/narrators/293/  sharon tanagi aburano   \n",
            "4  https://ddr.densho.org/narrators/597/        toshiko aiboshi   \n",
            "\n",
            "                                         cleaned_bio  \\\n",
            "0  nisei femal born may selleck washington spent ...   \n",
            "1  nisei male born june seattl washington grew ar...   \n",
            "2  nisei femal born octob seattl washington famil...   \n",
            "3  nisei femal born octob seattl washington famil...   \n",
            "4  nisei femal born juli boyl height california e...   \n",
            "\n",
            "                cleaned_interview_details  \n",
            "0        decemb seattl washington segment  \n",
            "1       januari seattl washington segment  \n",
            "2         april seattl washington segment  \n",
            "3         march seattl washington segment  \n",
            "4  januari culver citi california segment  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from spacy import displacy\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/narrators_data_cleaned.csv')\n",
        "\n",
        "bio = df['bio'].dropna()\n",
        "cleaned_bio = df['cleaned_bio'].dropna()\n",
        "\n",
        "df['original_noun_count'] = 0\n",
        "df['original_verb_count'] = 0\n",
        "df['original_adj_count'] = 0\n",
        "df['original_adv_count'] = 0\n",
        "\n",
        "df['cleaned_noun_count'] = 0\n",
        "df['cleaned_verb_count'] = 0\n",
        "df['cleaned_adj_count'] = 0\n",
        "df['cleaned_adv_count'] = 0\n",
        "\n",
        "for index, (original_text, cleaned_text) in enumerate(zip(bio, cleaned_bio)):\n",
        "    noun_count = 0\n",
        "    verb_count = 0\n",
        "    adj_count = 0\n",
        "    adv_count = 0\n",
        "\n",
        "    cleaned_noun_count = 0\n",
        "    cleaned_verb_count = 0\n",
        "    cleaned_adj_count = 0\n",
        "    cleaned_adv_count = 0\n",
        "\n",
        "    doc = nlp(original_text)\n",
        "\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            noun_count += 1\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            verb_count += 1\n",
        "        elif token.pos_ == \"ADJ\":\n",
        "            adj_count += 1\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            adv_count += 1\n",
        "\n",
        "    cleaned_doc = nlp(cleaned_text)\n",
        "\n",
        "    for token in cleaned_doc:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            cleaned_noun_count += 1\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            cleaned_verb_count += 1\n",
        "        elif token.pos_ == \"ADJ\":\n",
        "            cleaned_adj_count += 1\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            cleaned_adv_count += 1\n",
        "\n",
        "    df.at[index, 'original_noun_count'] = noun_count\n",
        "    df.at[index, 'original_verb_count'] = verb_count\n",
        "    df.at[index, 'original_adj_count'] = adj_count\n",
        "    df.at[index, 'original_adv_count'] = adv_count\n",
        "\n",
        "    df.at[index, 'cleaned_noun_count'] = cleaned_noun_count\n",
        "    df.at[index, 'cleaned_verb_count'] = cleaned_verb_count\n",
        "    df.at[index, 'cleaned_adj_count'] = cleaned_adj_count\n",
        "    df.at[index, 'cleaned_adv_count'] = cleaned_adv_count\n",
        "\n",
        "print(df[['bio', 'cleaned_bio', 'original_noun_count', 'original_verb_count', 'original_adj_count', 'original_adv_count',\n",
        "          'cleaned_noun_count', 'cleaned_verb_count', 'cleaned_adj_count', 'cleaned_adv_count']].head())\n",
        "\n",
        "df.to_csv('updated_narrators_data.csv', index=False)\n",
        "df1 = (\"updated_narrators_data.csv\")\n",
        "print(df1.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUKwv61zlnRU",
        "outputId": "acb6ea1e-9579-491c-adb0-395b0d59a510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 bio  \\\n",
            "0  Nisei female. Born May 9, 1927, in Selleck, Wa...   \n",
            "1  Nisei male. Born June 12, 1921, in Seattle, Wa...   \n",
            "2  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "3  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "4  Nisei female. Born July 8, 1928, in Boyle Heig...   \n",
            "\n",
            "                                         cleaned_bio  original_noun_count  \\\n",
            "0  nisei femal born may selleck washington spent ...                   12   \n",
            "1  nisei male born june seattl washington grew ar...                   16   \n",
            "2  nisei femal born octob seattl washington famil...                   10   \n",
            "3  nisei femal born octob seattl washington famil...                   10   \n",
            "4  nisei femal born juli boyl height california e...                   13   \n",
            "\n",
            "   original_verb_count  original_adj_count  original_adv_count  \\\n",
            "0                    8                   4                   0   \n",
            "1                   14                   8                   2   \n",
            "2                    8                   3                   2   \n",
            "3                    8                   3                   2   \n",
            "4                   10                   3                   2   \n",
            "\n",
            "   cleaned_noun_count  cleaned_verb_count  cleaned_adj_count  \\\n",
            "0                  11                   3                  2   \n",
            "1                  15                   8                  4   \n",
            "2                  11                   4                  3   \n",
            "3                  11                   4                  3   \n",
            "4                  10                   4                  4   \n",
            "\n",
            "   cleaned_adv_count  \n",
            "0                  0  \n",
            "1                  1  \n",
            "2                  0  \n",
            "3                  0  \n",
            "4                  1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('/content/narrators_data_cleaned.csv')\n",
        "\n",
        "bio = df['bio'].dropna()\n",
        "cleaned_bio = df['cleaned_bio'].dropna()\n",
        "for index, (original_text, cleaned_text) in enumerate(zip(bio, cleaned_bio)):\n",
        "\n",
        "    original_doc = nlp(original_text)\n",
        "\n",
        "    print(f\"\\nDependency Parse Tree for Original Bio (Index {index}):\")\n",
        "    displacy.render(original_doc, style=\"dep\", page=True)\n",
        "\n",
        "    cleaned_doc = nlp(cleaned_text)\n",
        "\n",
        "    print(f\"\\nDependency Parse Tree for Cleaned Bio (Index {index}):\")\n",
        "    displacy.render(cleaned_doc, style=\"dep\", page=True)\n",
        "\n",
        "\n",
        "    print(f\"\\nConstituency Parse Tree for Original Bio (Index {index}):\")\n",
        "    displacy.render(original_doc, style=\"dep\", page=True)\n",
        "\n",
        "    print(f\"\\nConstituency Parse Tree for Cleaned Bio (Index {index}):\")\n",
        "    displacy.render(cleaned_doc, style=\"dep\", page=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uFLi7RPM1YRO",
        "outputId": "4195a368-7bb6-447b-eef3-9ef05347f39a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/narrators_data_cleaned.csv')\n",
        "\n",
        "# Extract relevant columns\n",
        "bio = df['bio'].dropna()\n",
        "cleaned_bio = df['cleaned_bio'].dropna()\n",
        "\n",
        "# Add new columns for entity counts\n",
        "df['person_count'] = 0\n",
        "df['organization_count'] = 0\n",
        "df['location_count'] = 0\n",
        "df['product_count'] = 0\n",
        "df['date_count'] = 0\n",
        "\n",
        "# Loop over the rows and extract entities\n",
        "for index, (original_text, cleaned_text) in enumerate(zip(bio, cleaned_bio)):\n",
        "    # Initialize counters\n",
        "    person_count = 0\n",
        "    organization_count = 0\n",
        "    location_count = 0\n",
        "    product_count = 0\n",
        "    date_count = 0\n",
        "\n",
        "    # Process original text\n",
        "    doc = nlp(original_text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'PERSON':\n",
        "            person_count += 1\n",
        "        elif ent.label_ == 'ORG':\n",
        "            organization_count += 1\n",
        "        elif ent.label_ == 'GPE':  # GPE = Geo-Political Entity (locations, countries, cities)\n",
        "            location_count += 1\n",
        "        elif ent.label_ == 'PRODUCT':\n",
        "            product_count += 1\n",
        "        elif ent.label_ == 'DATE':\n",
        "            date_count += 1\n",
        "\n",
        "    # Process cleaned text\n",
        "    cleaned_doc = nlp(cleaned_text)\n",
        "    for ent in cleaned_doc.ents:\n",
        "        if ent.label_ == 'PERSON':\n",
        "            person_count += 1\n",
        "        elif ent.label_ == 'ORG':\n",
        "            organization_count += 1\n",
        "        elif ent.label_ == 'GPE':\n",
        "            location_count += 1\n",
        "        elif ent.label_ == 'PRODUCT':\n",
        "            product_count += 1\n",
        "        elif ent.label_ == 'DATE':\n",
        "            date_count += 1\n",
        "\n",
        "    # Assign the counts to the dataframe\n",
        "    df.at[index, 'person_count'] = person_count\n",
        "    df.at[index, 'organization_count'] = organization_count\n",
        "    df.at[index, 'location_count'] = location_count\n",
        "    df.at[index, 'product_count'] = product_count\n",
        "    df.at[index, 'date_count'] = date_count\n",
        "\n",
        "# Print the updated dataframe with entity counts\n",
        "print(df[['bio', 'cleaned_bio', 'person_count', 'organization_count', 'location_count', 'product_count', 'date_count']].head())\n",
        "\n",
        "# Save the updated dataframe to a CSV file\n",
        "df.to_csv('updated_narrators_data_with_entities.csv', index=False)\n",
        "\n",
        "# Optional: load and print the saved file for verification\n",
        "df1 = pd.read_csv(\"updated_narrators_data_with_entities.csv\")\n",
        "print(df1.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gVitiyG1vfy",
        "outputId": "d9bda14c-db61-419f-bcd3-3664e20544a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 bio  \\\n",
            "0  Nisei female. Born May 9, 1927, in Selleck, Wa...   \n",
            "1  Nisei male. Born June 12, 1921, in Seattle, Wa...   \n",
            "2  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "3  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "4  Nisei female. Born July 8, 1928, in Boyle Heig...   \n",
            "\n",
            "                                         cleaned_bio  person_count  \\\n",
            "0  nisei femal born may selleck washington spent ...             2   \n",
            "1  nisei male born june seattl washington grew ar...             4   \n",
            "2  nisei femal born octob seattl washington famil...             2   \n",
            "3  nisei femal born octob seattl washington famil...             2   \n",
            "4  nisei femal born juli boyl height california e...             4   \n",
            "\n",
            "   organization_count  location_count  product_count  date_count  \n",
            "0                   2              12              0           1  \n",
            "1                   8              11              0           2  \n",
            "2                   3              12              0           2  \n",
            "3                   3              12              0           2  \n",
            "4                   2               9              0           1  \n",
            "                    name                                                bio  \\\n",
            "0           Kay Aiko Abe  Nisei female. Born May 9, 1927, in Selleck, Wa...   \n",
            "1                Art Abe  Nisei male. Born June 12, 1921, in Seattle, Wa...   \n",
            "2  Sharon Tanagi Aburano  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "3  Sharon Tanagi Aburano  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "4        Toshiko Aiboshi  Nisei female. Born July 8, 1928, in Boyle Heig...   \n",
            "\n",
            "                                     interview_title  \\\n",
            "0       Kay Aiko Abe Interview — ddr-densho-1000-232   \n",
            "1            Art Abe Interview — ddr-densho-1000-206   \n",
            "2  Sharon Tanagi Aburano Interview II — ddr-densh...   \n",
            "3  Sharon Tanagi Aburano Interview I — ddr-densho...   \n",
            "4         Toshiko Aiboshi Interview — ddr-manz-1-112   \n",
            "\n",
            "                                   interview_details  \\\n",
            "0  December 2, 2008.\\n      Seattle, Washington.0...   \n",
            "1  January 24, 2008.\\n      Seattle, Washington.0...   \n",
            "2  April 3, 2008.\\n      Seattle, Washington.02:2...   \n",
            "3  March 25, 2008.\\n      Seattle, Washington.02:...   \n",
            "4  January 20, 2011.\\n      Culver City, Californ...   \n",
            "\n",
            "                                interview_link  \\\n",
            "0  https://ddr.densho.org/ddr-densho-1000-232/   \n",
            "1  https://ddr.densho.org/ddr-densho-1000-206/   \n",
            "2  https://ddr.densho.org/ddr-densho-1000-209/   \n",
            "3  https://ddr.densho.org/ddr-densho-1000-208/   \n",
            "4       https://ddr.densho.org/ddr-manz-1-112/   \n",
            "\n",
            "                            narrator_url           cleaned_name  \\\n",
            "0  https://ddr.densho.org/narrators/361/           kay aiko abe   \n",
            "1  https://ddr.densho.org/narrators/291/                art abe   \n",
            "2  https://ddr.densho.org/narrators/293/  sharon tanagi aburano   \n",
            "3  https://ddr.densho.org/narrators/293/  sharon tanagi aburano   \n",
            "4  https://ddr.densho.org/narrators/597/        toshiko aiboshi   \n",
            "\n",
            "                                         cleaned_bio  \\\n",
            "0  nisei femal born may selleck washington spent ...   \n",
            "1  nisei male born june seattl washington grew ar...   \n",
            "2  nisei femal born octob seattl washington famil...   \n",
            "3  nisei femal born octob seattl washington famil...   \n",
            "4  nisei femal born juli boyl height california e...   \n",
            "\n",
            "                cleaned_interview_details  person_count  organization_count  \\\n",
            "0        decemb seattl washington segment             2                   2   \n",
            "1       januari seattl washington segment             4                   8   \n",
            "2         april seattl washington segment             2                   3   \n",
            "3         march seattl washington segment             2                   3   \n",
            "4  januari culver citi california segment             4                   2   \n",
            "\n",
            "   location_count  product_count  date_count  \n",
            "0              12              0           1  \n",
            "1              11              0           2  \n",
            "2              12              0           2  \n",
            "3              12              0           2  \n",
            "4               9              0           1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcVqy1yj3wja"
      },
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEdcyHX8VaDB"
      },
      "source": [
        "#Question 4 (20 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ung5_YW3C6y"
      },
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTOfUpatronW"
      },
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part - 1"
      ],
      "metadata": {
        "id": "-CSboqQG7pIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "main_url = \"https://github.com/marketplace?type=actions&page={}\"\n",
        "\n",
        "\n",
        "products_info = []\n",
        "\n",
        "\n",
        "for page_num in range(1, 55):\n",
        "    link1 = Request(main_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "\n",
        "    retry_count = 0\n",
        "    while retry_count < 5:\n",
        "        try:\n",
        "            url1 = urlopen(link1)\n",
        "            data1 = url1.read()\n",
        "            data1_soup = BeautifulSoup(data1, 'html.parser')\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred while fetching page {page_num}: {e}\")\n",
        "            retry_count += 1\n",
        "            time.sleep(random.uniform(2, 5))\n",
        "    else:\n",
        "        print(f\"Failed to retrieve page {page_num} after multiple attempts.\")\n",
        "        continue\n",
        "\n",
        "\n",
        "    products = data1_soup.find_all('div', class_=\"d-flex flex-justify-between flex-items-start gap-3\")\n",
        "\n",
        "    for product in products:\n",
        "\n",
        "        name_tag = product.find('h3', class_=\"d-flex f4 lh-condensed prc-Heading-Heading-6CmGO\")\n",
        "        name = name_tag.get_text(strip=True) if name_tag else \"No name available.\"\n",
        "\n",
        "\n",
        "        url_tag = product.find('a')\n",
        "        url = f\"https://github.com{url_tag['href']}\" if url_tag else None\n",
        "\n",
        "        description_text = \"No description available.\"\n",
        "        if url:\n",
        "            retry_count = 0\n",
        "            while retry_count < 5:\n",
        "                try:\n",
        "                    link2 = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "                    url2 = urlopen(link2)\n",
        "                    data2 = url2.read()\n",
        "                    data2_soup = BeautifulSoup(data2, 'html.parser')\n",
        "\n",
        "                    description_taga = data2_soup.find('div', class_=\"Box-sc-g0xbh4-0 gpEbNC\")\n",
        "                    description_tagb = description_taga.find('div', class_=\"Stack__StyledStack-sc-x3xa2i-0 hyhjyo\") if description_taga else None\n",
        "                    description_tag = description_tagb.find('span') if description_tagb else None\n",
        "                    description_text = description_tag.get_text() if description_tag else \"No description available.\"\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error occurred while fetching description for {url}: {e}\")\n",
        "                    retry_count += 1\n",
        "                    time.sleep(random.uniform(2, 5))\n",
        "            else:\n",
        "                print(f\"Failed to retrieve description for {url} after multiple attempts.\")\n",
        "\n",
        "        products_info.append({\n",
        "            'name': name,\n",
        "            'url': url,\n",
        "            'description': description_text,\n",
        "            'page_number': page_num\n",
        "        })\n",
        "\n",
        "    time.sleep(random.uniform(3, 6))\n",
        "\n",
        "\n",
        "with open('products_info.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=['name', 'url', 'description', 'page_number'])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(products_info)\n"
      ],
      "metadata": {
        "id": "aon2MMuR2qMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/products_info.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "xYbnF0AhU3jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part - 2"
      ],
      "metadata": {
        "id": "k5UdK2WP7tcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "aFnES6slXVxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    tokens = text.split()\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "df['cleaned_name'] = df['name'].apply(preprocess)\n",
        "df['cleaned_description'] = df['description'].apply(preprocess)\n",
        "df['cleaned_url'] = df['url'].apply(preprocess)\n",
        "\n",
        "print(df[['name', 'cleaned_name', 'url', 'cleaned_url','description','cleaned_description']].head())\n"
      ],
      "metadata": {
        "id": "btckWnEANw8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Quality\n"
      ],
      "metadata": {
        "id": "RsLCzJ_Ygpso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "\n",
        "\n",
        "df = pd.read_csv('products_info.csv')\n",
        "\n",
        "print(\"Checking for missing values...\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "df = df.dropna(subset=['name', 'url', 'description', 'page_number'])\n",
        "\n",
        "print(\"Checking for duplicate entries...\")\n",
        "df_duplicates = df[df.duplicated(subset=['url'], keep='first')]  # Keep the first occurrence of each duplicate\n",
        "if not df_duplicates.empty:\n",
        "    print(f\"Found {len(df_duplicates)} duplicate entries. Removing duplicates.\")\n",
        "    df = df.drop_duplicates(subset=['url'], keep='first')\n",
        "else:\n",
        "    print(\"No duplicates found.\")\n",
        "\n",
        "print(\"Validating URLs...\")\n",
        "valid_urls = df['url'].apply(lambda x: bool(re.match(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', x)))\n",
        "invalid_urls = df[~valid_urls]  # Get rows with invalid URLs\n",
        "if not invalid_urls.empty:\n",
        "    print(f\"Found {len(invalid_urls)} rows with invalid URLs. Removing them.\")\n",
        "    df = df[valid_urls]  # Remove invalid URLs\n",
        "else:\n",
        "    print(\"All URLs are valid.\")\n",
        "\n",
        "print(\"Validating 'page_number'...\")\n",
        "df['page_number'] = pd.to_numeric(df['page_number'], errors='coerce')  # Convert to numeric, invalid values will become NaN\n",
        "df = df.dropna(subset=['page_number'])  # Remove rows where 'page_number' is NaN\n",
        "\n",
        "\n",
        "df.to_csv('products_info_cleaned.csv', index=False)\n",
        "\n",
        "print(\"\\nCleaned data sample:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nData Quality Check Summary:\")\n",
        "print(f\"Total rows after cleaning: {len(df)}\")\n",
        "print(f\"Missing values after cleaning: {df.isnull().sum()}\")\n"
      ],
      "metadata": {
        "id": "tPmwqkLwgujr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WeD70ty3Gui"
      },
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tweepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc97jWnhK3Fm",
        "outputId": "638f3b3c-e9e5-416c-8f5f-3900ee9833cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "bearer_token = 'AAAAAAAAAAAAAAAAAAAAALilzQEAAAAAkcarJ3HmwmE6Nj2bp5ny59hPVik%3DNWgabd8iSYP4PiWmkvlWPyp8l341IsHO5ibljBVR6w1adoJkCw'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZqqE0tWf-iJ",
        "outputId": "30de082b-0b4e-41ca-d700-ccd89ab36ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "query = \"Machine Learning\"\n",
        "tweets = client.search_recent_tweets(query=query, tweet_fields=[\"created_at\", \"text\", \"author_id\"], max_results=100)\n",
        "\n",
        "tweet_data = []\n",
        "\n",
        "if tweets.data:\n",
        "    for tweet in tweets.data:\n",
        "        tweet_data.append({\n",
        "            'tweet_id': tweet.id,\n",
        "            'author_id': tweet.author_id,\n",
        "            'text': tweet.text\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(tweet_data)\n",
        "print(df)\n",
        "\n",
        "\n",
        "df.to_csv(\"tweets_llm.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ48BjnFf6Jc",
        "outputId": "20870346-8121-45f7-e260-d99b7d613024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               tweet_id            author_id  \\\n",
            "0   1892423053352595833   961934520202158080   \n",
            "1   1892422928517808322  1381957245915762690   \n",
            "2   1892422681623957983   788898706586275840   \n",
            "3   1892422602104143897  1855984825514991616   \n",
            "4   1892422519321174453             53103453   \n",
            "..                  ...                  ...   \n",
            "95  1892416092875763890  1867900705157292032   \n",
            "96  1892416081026855142            318906105   \n",
            "97  1892416053004632377  1757830802979565568   \n",
            "98  1892416050270023899  1784204075720212480   \n",
            "99  1892416047581483046  1858996603673014272   \n",
            "\n",
            "                                                 text  \n",
            "0   RT @SakanaAILabs: Introducing The AI CUDA Engi...  \n",
            "1   Need help with #Assignment Essay #Homework wri...  \n",
            "2   Become a TDS contributor 📢 Share your insights...  \n",
            "3   @refaee_abd we could trace how babylonian astr...  \n",
            "4   RT @SakanaAILabs: Introducing The AI CUDA Engi...  \n",
            "..                                                ...  \n",
            "95  RT @DannyOfCrypto: ➥ How ZkAGI Works\\n\\n@zk_ag...  \n",
            "96  @opus_genesis AI systems can learn and improve...  \n",
            "97  RT @0xnet_official: 🚀0xNet is designed for mac...  \n",
            "98  RT @0xnet_official: 🚀0xNet is designed for mac...  \n",
            "99  RT @0xnet_official: 🚀0xNet is designed for mac...  \n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/tweets_llm.csv\")"
      ],
      "metadata": {
        "id": "UYWa6IfRimc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = df.isnull().sum()\n",
        "print(f\"Missing values before cleaning:\\n{missing_values}\\n\")\n",
        "\n",
        "\n",
        "duplicates = df[df.duplicated()]\n",
        "print(f\"Duplicates before cleaning:\\n{duplicates}\\n\")\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "df['text'] = df['text'].str.strip()\n",
        "df['text'] = df['text'].str.replace('\\n', ' ', regex=False)\n",
        "\n",
        "\n",
        "if not df['tweet_id'].apply(lambda x: isinstance(x, int)).all():\n",
        "    print(\"Warning: Some tweet_id values are not integers.\")\n",
        "if not df['author_id'].apply(lambda x: isinstance(x, int)).all():\n",
        "    print(\"Warning: Some author_id values are not integers.\")\n",
        "\n",
        "\n",
        "missing_values_after = df.isnull().sum()\n",
        "print(f\"Missing values after cleaning:\\n{missing_values_after}\\n\")\n",
        "print(\"Cleaned DataFrame:\")\n",
        "print(df)\n",
        "\n",
        "\n",
        "output_file = 'twitter_cleaned_data.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Cleaned data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q90Re6ULjfqc",
        "outputId": "e31b61ca-f270-4955-dc12-aaa34e35e399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values before cleaning:\n",
            "tweet_id     0\n",
            "author_id    0\n",
            "text         0\n",
            "dtype: int64\n",
            "\n",
            "Duplicates before cleaning:\n",
            "Empty DataFrame\n",
            "Columns: [tweet_id, author_id, text]\n",
            "Index: []\n",
            "\n",
            "Missing values after cleaning:\n",
            "tweet_id     0\n",
            "author_id    0\n",
            "text         0\n",
            "dtype: int64\n",
            "\n",
            "Cleaned DataFrame:\n",
            "               tweet_id            author_id  \\\n",
            "0   1892423053352595833   961934520202158080   \n",
            "1   1892422928517808322  1381957245915762690   \n",
            "2   1892422681623957983   788898706586275840   \n",
            "3   1892422602104143897  1855984825514991616   \n",
            "4   1892422519321174453             53103453   \n",
            "..                  ...                  ...   \n",
            "95  1892416092875763890  1867900705157292032   \n",
            "96  1892416081026855142            318906105   \n",
            "97  1892416053004632377  1757830802979565568   \n",
            "98  1892416050270023899  1784204075720212480   \n",
            "99  1892416047581483046  1858996603673014272   \n",
            "\n",
            "                                                 text  \n",
            "0   RT @SakanaAILabs: Introducing The AI CUDA Engi...  \n",
            "1   Need help with #Assignment Essay #Homework wri...  \n",
            "2   Become a TDS contributor 📢 Share your insights...  \n",
            "3   @refaee_abd we could trace how babylonian astr...  \n",
            "4   RT @SakanaAILabs: Introducing The AI CUDA Engi...  \n",
            "..                                                ...  \n",
            "95  RT @DannyOfCrypto: ➥ How ZkAGI Works  @zk_agi ...  \n",
            "96  @opus_genesis AI systems can learn and improve...  \n",
            "97  RT @0xnet_official: 🚀0xNet is designed for mac...  \n",
            "98  RT @0xnet_official: 🚀0xNet is designed for mac...  \n",
            "99  RT @0xnet_official: 🚀0xNet is designed for mac...  \n",
            "\n",
            "[100 rows x 3 columns]\n",
            "Cleaned data saved to twitter_cleaned_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbTa-jDS-KFI"
      },
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}